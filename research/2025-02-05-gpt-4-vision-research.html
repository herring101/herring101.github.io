<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/92f44bb82993d879-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/d5495b0411c8fb4f.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-1e391cdf0fe59d34.js" crossorigin=""/><script src="/_next/static/chunks/b40443eb-b9fe3fcc7373e7da.js" async="" crossorigin=""></script><script src="/_next/static/chunks/848-3ff3edc57ae3c83d.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-aa12d8350e658c2e.js" async="" crossorigin=""></script><script src="/_next/static/chunks/765-9a51a052c0b92136.js" async=""></script><script src="/_next/static/chunks/605-4a7004ddde8f7487.js" async=""></script><script src="/_next/static/chunks/887-f00c177c05ba72ba.js" async=""></script><script src="/_next/static/chunks/app/layout-e44ec4ebe3a575a2.js" async=""></script><title>Herring&#x27;s Portfolio</title><meta name="description" content="A portfolio showcasing my journey in AI and human intelligence research"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="__variable_d65c78 __variable_43fb55 min-h-screen bg-background font-sans antialiased"><script>((e,t,r,n,o,l,a,i)=>{let u=document.documentElement,s=["light","dark"];function c(t){(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&l?o.map(e=>l[e]||e):o;r?(u.classList.remove(...n),u.classList.add(t)):u.setAttribute(e,t)}),i&&s.includes(t)&&(u.style.colorScheme=t)}if(n)c(n);else try{let e=localStorage.getItem(t)||r,n=a&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;c(n)}catch(e){}})("class","theme","dark",null,["light","dark"],null,true,true)</script><div class="relative flex min-h-screen flex-col"><header class="sticky top-0 z-50 w-full border-b bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60"><div class="container flex h-14 items-center max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex w-full items-center justify-between"><a class="flex items-center space-x-2 transition-colors hover:text-primary" href="/"><span class="inline-block text-lg font-bold tracking-tight">Herring</span></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" class="relative z-10 flex max-w-max flex-1 items-center justify-center"><div style="position:relative"><ul data-orientation="horizontal" class="group flex flex-1 list-none items-center justify-center space-x-1" dir="ltr"><li><button id="radix-:R17la:-trigger-radix-:Rn7la:" data-state="closed" aria-expanded="false" aria-controls="radix-:R17la:-content-radix-:Rn7la:" class="group inline-flex h-10 w-max items-center justify-center rounded-md bg-background px-4 py-2 text-sm font-medium transition-colors hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground focus:outline-none disabled:pointer-events-none disabled:opacity-50 data-[active]:bg-accent/50 data-[state=open]:bg-accent/50 group" data-radix-collection-item="">Explore<!-- --> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down relative top-[1px] ml-1 h-3 w-3 transition duration-200 group-data-[state=open]:rotate-180" aria-hidden="true"><path d="m6 9 6 6 6-6"></path></svg></button></li><li><a class="group inline-flex h-10 w-max items-center justify-center rounded-md bg-background/50 px-4 py-2 text-sm font-medium transition-colors hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground focus:outline-none disabled:pointer-events-none disabled:opacity-50" href="/about" data-radix-collection-item="">About</a></li></ul></div><div class="absolute left-0 top-full flex justify-center"></div></nav><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-9 w-9 px-0"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-sun h-[1.2rem] w-[1.2rem] rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0"><circle cx="12" cy="12" r="4"></circle><path d="M12 2v2"></path><path d="M12 20v2"></path><path d="m4.93 4.93 1.41 1.41"></path><path d="m17.66 17.66 1.41 1.41"></path><path d="M2 12h2"></path><path d="M20 12h2"></path><path d="m6.34 17.66-1.41 1.41"></path><path d="m19.07 4.93-1.41 1.41"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon absolute h-[1.2rem] w-[1.2rem] rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg><span class="sr-only">Toggle theme</span></button></div></div></header><main class="flex-1 container max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8 py-8"><article class="container mx-auto py-8 max-w-4xl"><div class="mb-8"><h1 class="text-4xl font-bold mb-4">GPT-4 Visionを用いた視覚的理解の研究</h1><div class="flex items-center gap-4 mb-4"><time class="text-muted-foreground">2025/2/5</time><div class="flex gap-2 flex-wrap"><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80">GPT-4</div><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80">Computer Vision</div><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80">Deep Learning</div><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80">OpenAI</div></div></div><p class="text-xl text-muted-foreground">GPT-4 Visionの視覚的理解能力と応用可能性について深く掘り下げた研究</p></div><div class="prose prose-lg dark:prose-invert max-w-none"><h1>GPT-4 Visionを用いた視覚的理解の研究</h1>
<h2>はじめに</h2>
<p>GPT-4 Visionは、テキストと画像を組み合わせた理解を可能にする革新的なモデルです。
本研究では、このモデルの視覚的理解能力と実世界での応用可能性について詳しく検討します。</p>
<h2>研究方法</h2>
<p>我々は以下の3つの観点から評価を行いました：</p>
<ol>
<li>物体認識の精度</li>
<li>文脈理解の深さ</li>
<li>マルチモーダル推論能力</li>
</ol>
<h2>主な発見</h2>
<p>研究の結果、以下の重要な知見が得られました：</p>
<ul>
<li>GPT-4 Visionは従来のコンピュータビジョンモデルを大きく上回る文脈理解能力を示した</li>
<li>画像内の複数の要素間の関係性を理解し、適切な推論が可能</li>
<li>特に医療画像分析において、専門家レベルの判断能力を示す場面も</li>
</ul>
<h2>今後の展望</h2>
<p>本研究の成果は、以下の分野での応用が期待されます：</p>
<ul>
<li>医療診断支援</li>
<li>自動運転システム</li>
<li>セキュリティ監視</li>
<li>教育支援ツール</li>
</ul>
<h2>結論</h2>
<p>GPT-4 Visionは、人工知能の視覚的理解において新たな地平を切り開く可能性を示しています。
今後の研究開発により、さらなる進展が期待されます。</p></div></article></main></div><script src="/_next/static/chunks/webpack-1e391cdf0fe59d34.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/92f44bb82993d879-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/d5495b0411c8fb4f.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L4\"\n"])</script><script>self.__next_f.push([1,"5:I[2386,[],\"\"]\n8:I[9261,[],\"\"]\na:I[6675,[],\"\"]\nb:I[8379,[\"765\",\"static/chunks/765-9a51a052c0b92136.js\",\"605\",\"static/chunks/605-4a7004ddde8f7487.js\",\"887\",\"static/chunks/887-f00c177c05ba72ba.js\",\"185\",\"static/chunks/app/layout-e44ec4ebe3a575a2.js\"],\"Providers\"]\nc:I[5861,[\"765\",\"static/chunks/765-9a51a052c0b92136.js\",\"605\",\"static/chunks/605-4a7004ddde8f7487.js\",\"887\",\"static/chunks/887-f00c177c05ba72ba.js\",\"185\",\"static/chunks/app/layout-e44ec4ebe3a575a2.js\"],\"MainNav\"]\ne:I[5404,[],\"\"]\n9:[\"slug\",\"2025-02-0"])</script><script>self.__next_f.push([1,"5-gpt-4-vision-research\",\"d\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"4:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/d5495b0411c8fb4f.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"sINZ3LfkOSb8c3s1jZWdK\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/research/2025-02-05-gpt-4-vision-research\",\"initialTree\":[\"\",{\"children\":[\"research\",{\"children\":[[\"slug\",\"2025-02-05-gpt-4-vision-research\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"2025-02-05-gpt-4-vision-research\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"research\",{\"children\":[[\"slug\",\"2025-02-05-gpt-4-vision-research\",\"d\"],{\"children\":[\"__PAGE__\",{},[\"$L6\",[\"$\",\"article\",null,{\"className\":\"container mx-auto py-8 max-w-4xl\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold mb-4\",\"children\":\"GPT-4 Visionを用いた視覚的理解の研究\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 mb-4\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-muted-foreground\",\"children\":\"2025/2/5\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-2 flex-wrap\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80\",\"children\":\"GPT-4\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80\",\"children\":\"Computer Vision\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80\",\"children\":\"Deep Learning\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80\",\"children\":\"OpenAI\"}]]}]]}],[\"$\",\"p\",null,{\"className\":\"text-xl text-muted-foreground\",\"children\":\"GPT-4 Visionの視覚的理解能力と応用可能性について深く掘り下げた研究\"}]]}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg dark:prose-invert max-w-none\",\"children\":\"$L7\"}]]}],null]]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"research\",\"children\",\"$9\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"research\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_d65c78 __variable_43fb55 min-h-screen bg-background font-sans antialiased\",\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"relative flex min-h-screen flex-col\",\"children\":[[\"$\",\"header\",null,{\"className\":\"sticky top-0 z-50 w-full border-b bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60\",\"children\":[\"$\",\"div\",null,{\"className\":\"container flex h-14 items-center max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8\",\"children\":[\"$\",\"$Lc\",null,{}]}]}],[\"$\",\"main\",null,{\"className\":\"flex-1 container max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8 py-8\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}]}]]}]}]}]}],null]],\"initialHead\":[false,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"h1\",null,{\"children\":\"GPT-4 Visionを用いた視覚的理解の研究\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"はじめに\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"GPT-4 Visionは、テキストと画像を組み合わせた理解を可能にする革新的なモデルです。\\n本研究では、このモデルの視覚的理解能力と実世界での応用可能性について詳しく検討します。\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"研究方法\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"我々は以下の3つの観点から評価を行いました：\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"物体認識の精度\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"文脈理解の深さ\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"マルチモーダル推論能力\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"主な発見\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"研究の結果、以下の重要な知見が得られました：\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"GPT-4 Visionは従来のコンピュータビジョンモデルを大きく上回る文脈理解能力を示した\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"画像内の複数の要素間の関係性を理解し、適切な推論が可能\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"特に医療画像分析において、専門家レベルの判断能力を示す場面も\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"今後の展望\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"本研究の成果は、以下の分野での応用が期待されます：\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"医療診断支援\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"自動運転システム\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"セキュリティ監視\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"教育支援ツール\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"結論\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"GPT-4 Visionは、人工知能の視覚的理解において新たな地平を切り開く可能性を示しています。\\n今後の研究開発により、さらなる進展が期待されます。\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Herring's Portfolio\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"A portfolio showcasing my journey in AI and human intelligence research\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>